{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logisitc Regression to analyze the positivity of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the tweet sample supplied by nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\howuseeit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import twitter_samples\n",
    "nltk.download(\"twitter_samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocessing the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read tweets into positive tweets list and negative tweets list\n",
    "positive_tweets = twitter_samples.strings(\"positive_tweets.json\")\n",
    "negative_tweets = twitter_samples.strings(\"negative_tweets.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split the train, test dataset with 4:1 portion\n",
    "\n",
    "X_train = positive_tweets[:4000] + negative_tweets[:4000]\n",
    "X_test  = positive_tweets[4000:] + negative_tweets[4000:]\n",
    "\n",
    "y_train = np.append(np.ones(len(positive_tweets[:4000])), np.zeros(len(negative_tweets[:4000])))\n",
    "y_test = np.append(np.ones(len(positive_tweets[4000:])), np.zeros(len(negative_tweets[4000:])))\n",
    "\n",
    "y_train = y_train.reshape(y_train.shape[0],1)\n",
    "y_test = y_test.reshape(y_test.shape[0],1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess the tweets by:\n",
    "##### 1. use regular expression to strip unneccessary texts\n",
    "##### 2. use stopwords by nltk to strip stopwords\n",
    "##### 3. use PorterStemmer by nltk to get the absolute base form of words\n",
    "##### 4. use nltk tokenizer to tokenize every tweets to list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\howuseeit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import re\n",
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tweet_process(tweets):\n",
    "    tweets_after_cleaning = []\n",
    "\n",
    "    for tweet in tweets:\n",
    "        \n",
    "        # stripping down hyperlinks, retweets indication, hashtags\n",
    "        tweet = re.sub(r'^RT[\\s]+', '',tweet)\n",
    "        tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*','',tweet)\n",
    "        tweet = re.sub(r'#','',tweet)\n",
    "        \n",
    "        #tokenizing tweets\n",
    "        tokenizer = TweetTokenizer(preserve_case = False,\n",
    "                                   strip_handles = True,\n",
    "                                   reduce_len = True)\n",
    "        \n",
    "        tweet = tokenizer.tokenize(tweet)\n",
    "        \n",
    "        #remove stopwords\n",
    "        for i in tweet:\n",
    "            if i in stopwords.words(\"english\") or i in string.punctuation:\n",
    "                    tweet.remove(i) \n",
    "                    \n",
    "         # Stemming: strip words to most general form\n",
    "        stemmer = PorterStemmer()\n",
    "        tweet = [stemmer.stem(i) for i in tweet]\n",
    "        tweets_after_cleaning.append(tweet)\n",
    "        \n",
    "    return tweets_after_cleaning   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a word frequency dictionary with (word, positivity) to track every word in a non-repetitive manner which will be used to count positivity of each words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def word_frequency(Y, tweets):\n",
    "    \n",
    "    #searh through each word in the preprocessed tweets \n",
    "    #and make a dictionary of each words\n",
    "    \n",
    "    ys = np.squeeze(Y).tolist()\n",
    "    freqs= {}\n",
    "    \n",
    "    for y,tweet in zip(ys, tweets):\n",
    "        for word in tweet:\n",
    "            \n",
    "            pair = (word, y)\n",
    "            \n",
    "            if pair in freqs:\n",
    "                freqs[pair] += 1\n",
    "            else:\n",
    "                freqs[pair] = 1\n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract features of a word by counting:\n",
    "##### 1. Bias 1\n",
    "##### 2. the time that it associates with a positive tweets\n",
    "##### 3. the time that it associates with a negative tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Extracting_features(tweets, freqs):\n",
    "    \n",
    "    x = np.zeros((len(tweets), 3))\n",
    "    \n",
    "    #set bias terms to 1\n",
    "    x[:,0] = 1\n",
    "    \n",
    "    for tweet in tweets:\n",
    "        for word in tweet:\n",
    "            x[tweets.index(tweet), 1] += freqs.get((word, 1.0), 0)\n",
    "            x[tweets.index(tweet), 2] += freqs.get((word, 0.0), 0)\n",
    "            \n",
    "    return x\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define sigmoid function\n",
    "##### Use Gradient Descent to optimize the theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    return 1/(1 + np.exp(-Z))\n",
    "\n",
    "def GradientDescent(x,y,theta,alpha,num_iters):\n",
    "    \n",
    "    m = x.shape[0]\n",
    "    for i in range(num_iters):\n",
    "        y_hat = sigmoid(np.dot(x,theta))\n",
    "\n",
    "        J = -1/m * (np.dot(y.transpose(), y_hat) + np.dot((1-y).transpose(),(1-y_hat)))\n",
    "    \n",
    "        theta = theta - alpha/m * np.dot(x.transpose(), (y_hat - y))\n",
    "\n",
    "    return float(J), theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tweets, freqs, theta):\n",
    "    \n",
    "    x = Extracting_features(tweets, freqs)\n",
    "    y_hat = sigmoid(np.dot(x,theta))\n",
    "    \n",
    "    return y_hat\n",
    "\n",
    "def accuracy(y_hat, y):\n",
    "    \n",
    "    predicts = []\n",
    "    for prediction in y_hat:\n",
    "        if prediction > 0.5:\n",
    "            predicts.append(1.0)\n",
    "        else:\n",
    "            predicts.append(0.0)\n",
    "            \n",
    "    accuracy = np.sum(np.array(predicts) == y.reshape(np.array(predicts).shape)) / len(predicts)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocess training tweets and test tweets\n",
    "X_train_post = tweet_process(X_train)\n",
    "X_test_post  = tweet_process(X_test)\n",
    "\n",
    "## Build the frequency dictionary for every word\n",
    "freqs = word_frequency(y_train, X_train_post)\n",
    "\n",
    "## Extract features from Training dataset\n",
    "features = Extracting_features(X_train_post,freqs)\n",
    "\n",
    "## Use Gradient Descent to train theta with alpha = 1e-8\n",
    "J, theta = GradientDescent(features, y_train, np.zeros((3,1)), 1e-8, 1500)\n",
    "\n",
    "## prediction of test data\n",
    "predicts = predict(X_test_post,freqs, theta)\n",
    "\n",
    "#accuracy\n",
    "accuracy(predicts, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
